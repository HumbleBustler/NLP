{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_24_Sep_2021_Evening.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1-otPj8oQdo"
      },
      "source": [
        "<h1> Preparing Text Data with scikit-learn </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtSejgZek648"
      },
      "source": [
        "1. Training a CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miBkl41qhVeL",
        "outputId": "e2587140-c4bf-47f6-feb0-f70ac6386fd1"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = CountVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(type(vector))\n",
        "print(vector.toarray())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "(1, 8)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "[[1 1 1 1 1 1 1 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtFHeCfclC0j"
      },
      "source": [
        "2. Encoding another document with the CountVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s3JLjumlIwC",
        "outputId": "336aae32-8d2a-4630-ddd6-e6f121a3fac5"
      },
      "source": [
        "# encode another document\n",
        "text2 = [\"the puppy\"]\n",
        "vector = vectorizer.transform(text2)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 0 0 0 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z87S7s6lN7t"
      },
      "source": [
        "3. Training a TfidfVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyn_J8rVlMmQ",
        "outputId": "a91d132b-a872-4a50-8e0f-641666aaa376"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
        "\"The dog.\",\n",
        "\"The fox\"]\n",
        "# create the transform\n",
        "vectorizer = TfidfVectorizer()\n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(text)\n",
        "# summarize\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.idf_)\n",
        "# encode document\n",
        "vector = vectorizer.transform([text[0]])\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
            " 1.69314718 1.        ]\n",
            "(1, 8)\n",
            "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
            "  0.36388646 0.42983441]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA31SmtDlWtS"
      },
      "source": [
        "4. Training a HashingVectorizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgVmlpv-lduG",
        "outputId": "e8590077-f62b-4f5a-9bf5-299e99ac70ef"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "# list of text documents\n",
        "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
        "# create the transform\n",
        "vectorizer = HashingVectorizer(n_features=20)\n",
        "# encode document\n",
        "vector = vectorizer.transform(text)\n",
        "# summarize encoded vector\n",
        "print(vector.shape)\n",
        "print(vector.toarray())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 20)\n",
            "[[ 0.          0.          0.          0.          0.          0.33333333\n",
            "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
            "   0.          0.          0.         -0.33333333  0.          0.\n",
            "  -0.66666667  0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-20ya2DRlg56"
      },
      "source": [
        "<h1>Preparing Text Data with Keras</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N03iVNmHlqf5"
      },
      "source": [
        "1. Splitting words with the Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teC5GuLFlrWg",
        "outputId": "a22e404c-9e23-41f3-cff2-88fd0f55cde5"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRTXZGSwlumL"
      },
      "source": [
        "2. Preparing a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gR9v1C6lykD",
        "outputId": "d6a28eef-94c0-48be-ee0d-b47bd661c11c"
      },
      "source": [
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hRFasz5l4C0"
      },
      "source": [
        "3. Using one hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MTUhMcGl9XZ",
        "outputId": "a3350189-7b57-446c-b5d4-ef66b69c04fb"
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = one_hot(text, round(vocab_size*1.3))\n",
        "print(result)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[7, 3, 5, 6, 9, 8, 7, 9, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJVp0AJXmA0X"
      },
      "source": [
        "5. Using hash encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Qk19eymBYk",
        "outputId": "725c6c90-66fe-4ce8-c9f0-62e085d21974"
      },
      "source": [
        "from keras.preprocessing.text import hashing_trick\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "# integer encode the document\n",
        "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
        "print(result)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFNcn56BmJK9"
      },
      "source": [
        "6. Fitting a Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQfX1MHpmKAy"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVmUywpLmN0P"
      },
      "source": [
        "7. Summarize the output of the fit Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY5XWdywmT4k",
        "outputId": "5d8ba644-3cd1-46ba-cdcc-3ee8936fe0b0"
      },
      "source": [
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "5\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZIpA1OimXC9"
      },
      "source": [
        "8. Fitting and encoding with the Tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5kbqQeEma5I",
        "outputId": "4786026f-10bb-404f-861e-3aeb3a1a340f"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        "'Good work',\n",
        "'Great effort',\n",
        "'nice work',\n",
        "'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "5\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "defaultdict(<class 'int'>, {'done': 1, 'well': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1})\n",
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    }
  ]
}